{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Análisis de un log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/30 23:39:51 WARN Utils: Your hostname, jesus-Aspire-A514-52 resolves to a loopback address: 127.0.1.1; using 192.168.1.54 instead (on interface wlp2s0)\n",
      "21/11/30 23:39:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/30 23:39:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/11/30 23:39:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('rdd-example').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el RDD leyendo el fichero de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logFile = sc.textFile(\"LabData/notebook.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color: red\">DEBES ESCRIBIR TU CÓDIGO DONDE LO INDIQUE:</span> \n",
    "\n",
    "#### En la celda de debajo, escribe el código necesario para filtrar todas las línesa que contiene la palabra \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info = logFile.filter(lambda line: \"INFO\" in line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Cuenta las líneas que hay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "13438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = info.count()\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Cuenta las líneas que contienen \"spark\" combinando una transformación y una acción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_lines = info.filter(lambda line: \"spark\" in line).count()\n",
    "spark_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Recupera esas líneas en un nuevo RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_rdd = info.filter(lambda line: \"spark\" in line).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Muestra el grafo (DAG) asociado al RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[5] at RDD at PythonRDD.scala:53 []\\n |  LabData/notebook.log MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  LabData/notebook.log HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Unir RDDs\n",
    "\n",
    "Vamos a crear dos RDDs para los ficheros README.md y POM.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readmeFile = sc.textFile(\"LabData/README.md\")\n",
    "pomFile = sc.textFile(\"LabData/pom.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar cuantas veces aparece la palabra \"Spark\" en cada fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_readme = readmeFile.filter(lambda line: \"spark\" in line).count()\n",
    "spark_pom = readmeFile.filter(lambda line: \"spark\" in line).count()\n",
    "\n",
    "spark_readme, spark_pom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contar el número de palabras distintas que aparecen en cada uno de los dos ficheros. Los resultados son RDDs de tipo PairRDD (Key, Value) de la forma (palabra, número de veces que aparece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "readmeCount = readmeFile.                    \\\n",
    "    flatMap(lambda line: line.split(\" \")).   \\\n",
    "    map(lambda word: (word, 1)).             \\\n",
    "    reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "pomCount = pomFile.                          \\\n",
    "    flatMap(lambda line: line.split(\" \")).   \\\n",
    "    map(lambda word: (word, 1)).            \\\n",
    "    reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar el RDD completo usando el método collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#', 1), ('Apache', 1), ('Spark', 14), ('', 67), ('is', 6), ('It', 2), ('provides', 1), ('high-level', 1), ('APIs', 1), ('in', 5), ('Scala,', 1), ('Java,', 1), ('an', 3), ('optimized', 1), ('engine', 1), ('supports', 2), ('computation', 1), ('analysis.', 1), ('set', 2), ('of', 5), ('tools', 1), ('SQL', 2), ('MLlib', 1), ('machine', 1), ('learning,', 1), ('GraphX', 1), ('graph', 1), ('processing,', 1), ('Documentation', 1), ('latest', 1), ('programming', 1), ('guide,', 1), ('[project', 2), ('README', 1), ('only', 1), ('basic', 1), ('instructions.', 1), ('Building', 1), ('using', 2), ('[Apache', 1), ('run:', 1), ('do', 2), ('this', 1), ('downloaded', 1), ('documentation', 3), ('project', 1), ('site,', 1), ('at', 2), ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1), ('Interactive', 2), ('Shell', 2), ('The', 1), ('way', 1), ('start', 1), ('Try', 1), ('following', 2), ('1000:', 2), ('scala>', 1), ('1000).count()', 1), ('Python', 2), ('Alternatively,', 1), ('use', 3), ('And', 1), ('run', 7), ('Example', 1), ('several', 1), ('programs', 2), ('them,', 1), ('`./bin/run-example', 1), ('[params]`.', 1), ('example:', 1), ('./bin/run-example', 2), ('SparkPi', 2), ('variable', 1), ('when', 1), ('examples', 2), ('spark://', 1), ('URL,', 1), ('YARN,', 1), ('\"local\"', 1), ('locally', 2), ('N', 1), ('abbreviated', 1), ('class', 2), ('name', 1), ('package.', 1), ('instance:', 1), ('print', 1), ('usage', 1), ('help', 1), ('no', 1), ('params', 1), ('are', 1), ('Testing', 1), ('Spark](#building-spark).', 1), ('Once', 1), ('built,', 1), ('tests', 2), ('using:', 1), ('./dev/run-tests', 1), ('Please', 3), ('guidance', 3), ('module,', 1), ('individual', 1), ('Note', 1), ('About', 1), ('uses', 1), ('library', 1), ('HDFS', 1), ('other', 1), ('Hadoop-supported', 1), ('storage', 1), ('systems.', 1), ('Because', 1), ('have', 1), ('changed', 1), ('different', 1), ('versions', 1), ('Hadoop,', 2), ('must', 1), ('against', 1), ('version', 1), ('refer', 2), ('particular', 3), ('distribution', 1), ('Hive', 2), ('Thriftserver', 1), ('distributions.', 1), ('[\"Third', 1), ('distribution.', 1), ('[Configuration', 1), ('Guide](http://spark.apache.org/docs/latest/configuration.html)', 1), ('online', 1), ('overview', 1), ('configure', 1), ('Spark.', 1), ('a', 10), ('fast', 1), ('and', 10), ('general', 2), ('cluster', 2), ('computing', 1), ('system', 1), ('for', 12), ('Big', 1), ('Data.', 1), ('Python,', 2), ('R,', 1), ('that', 3), ('graphs', 1), ('data', 1), ('also', 5), ('rich', 1), ('higher-level', 1), ('including', 3), ('DataFrames,', 1), ('Streaming', 1), ('stream', 1), ('processing.', 1), ('<http://spark.apache.org/>', 1), ('##', 8), ('Online', 1), ('You', 3), ('can', 6), ('find', 1), ('the', 21), ('documentation,', 1), ('on', 6), ('web', 1), ('page](http://spark.apache.org/documentation.html)', 1), ('wiki](https://cwiki.apache.org/confluence/display/SPARK).', 1), ('This', 2), ('file', 1), ('contains', 1), ('setup', 1), ('built', 1), ('Maven](http://maven.apache.org/).', 1), ('To', 2), ('build', 3), ('its', 1), ('example', 3), ('programs,', 1), ('build/mvn', 1), ('-DskipTests', 1), ('clean', 1), ('package', 1), ('(You', 1), ('not', 1), ('need', 1), ('to', 14), ('if', 4), ('you', 4), ('pre-built', 1), ('package.)', 1), ('More', 1), ('detailed', 2), ('available', 1), ('from', 1), ('[\"Building', 1), ('Scala', 2), ('easiest', 1), ('through', 1), ('shell:', 2), ('./bin/spark-shell', 1), ('command,', 2), ('which', 2), ('should', 2), ('return', 2), ('sc.parallelize(1', 1), ('prefer', 1), ('./bin/pyspark', 1), ('>>>', 1), ('sc.parallelize(range(1000)).count()', 1), ('Programs', 1), ('comes', 1), ('with', 4), ('sample', 1), ('`examples`', 2), ('directory.', 1), ('one', 2), ('<class>', 1), ('For', 2), ('will', 1), ('Pi', 1), ('locally.', 1), ('MASTER', 1), ('environment', 1), ('running', 1), ('submit', 1), ('cluster.', 1), ('be', 2), ('mesos://', 1), ('or', 3), ('\"yarn\"', 1), ('thread,', 1), ('\"local[N]\"', 1), ('threads.', 1), ('MASTER=spark://host:7077', 1), ('Many', 1), ('given.', 1), ('Running', 1), ('Tests', 1), ('first', 1), ('requires', 1), ('[building', 1), ('see', 1), ('how', 2), ('[run', 1), ('tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).', 1), ('A', 1), ('Hadoop', 4), ('Versions', 1), ('core', 1), ('talk', 1), ('protocols', 1), ('same', 1), ('your', 1), ('runs.', 1), ('[\"Specifying', 1), ('Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', 1), ('building', 3), ('See', 1), ('Party', 1), ('Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)', 1), ('application', 1), ('works', 1), ('Configuration', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(readmeCount.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<?xml', 1), ('version=\"1.0\"', 1), ('', 2931), ('Apache', 2), ('more', 1), ('NOTICE', 1), ('this', 3), ('work', 1), ('additional', 1), ('regarding', 1), ('copyright', 1), ('The', 2), ('2.0', 1), ('(the', 1), ('\"License\");', 1), ('may', 2), ('use', 1), ('in', 3), ('compliance', 1), ('License.', 2), ('obtain', 1), ('of', 2), ('at', 1), ('law', 1), ('is', 2), ('an', 1), ('\"AS', 1), ('IS\"', 1), ('BASIS,', 1), ('CONDITIONS', 1), ('OF', 1), ('KIND,', 1), ('specific', 1), ('language', 1), ('limitations', 1), ('-->', 7), ('xmlns=\"http://maven.apache.org/POM/4.0.0\"', 1), ('http://maven.apache.org/xsd/maven-4.0.0.xsd\">', 1), ('<modelVersion>4.0.0</modelVersion>', 1), ('<artifactId>spark-parent_2.10</artifactId>', 1), ('<relativePath>../pom.xml</relativePath>', 1), ('</parent>', 1), ('<sbt.project.name>examples</sbt.project.name>', 1), ('Project', 1), ('Examples</name>', 1), ('<dependencies>', 2), ('<version>${project.version}</version>', 12), ('<artifactId>spark-streaming_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-bagel_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-hive_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-graphx_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-flume_${scala.binary.version}</artifactId>', 1), ('<exclusions>', 6), ('<exclusion>', 35), ('<groupId>org.spark-project.protobuf</groupId>', 1), ('</exclusion>', 35), ('SPARK-4455', 4), ('<artifactId>hbase-protocol</artifactId>', 1), ('<artifactId>hbase-common</artifactId>', 1), ('<groupId>io.netty</groupId>', 2), ('<artifactId>netty</artifactId>', 2), ('<groupId>org.apache.hadoop</groupId>', 7), ('<artifactId>hadoop-core</artifactId>', 1), ('<artifactId>hadoop-client</artifactId>', 1), ('<artifactId>hadoop-mapreduce-client-jobclient</artifactId>', 1), ('<artifactId>hadoop-auth</artifactId>', 1), ('<artifactId>hbase-hadoop1-compat</artifactId>', 1), ('<artifactId>commons-math</artifactId>', 1), ('<artifactId>jersey-core</artifactId>', 2), ('<groupId>org.slf4j</groupId>', 1), ('<artifactId>jersey-server</artifactId>', 1), ('<artifactId>jersey-json</artifactId>', 1), ('uses', 1), ('better,', 1), ('but', 1), ('<groupId>commons-io</groupId>', 1), ('<artifactId>commons-io</artifactId>', 1), ('<scope>test</scope>', 2), ('<artifactId>commons-math3</artifactId>', 2), ('<groupId>com.twitter</groupId>', 1), ('<groupId>org.scalacheck</groupId>', 1), ('<artifactId>cassandra-all</artifactId>', 1), ('<version>1.2.6</version>', 1), ('<artifactId>guava</artifactId>', 1), ('<groupId>com.googlecode.concurrentlinkedhashmap</groupId>', 1), ('<artifactId>concurrentlinkedhashmap-lru</artifactId>', 1), ('<artifactId>commons-cli</artifactId>', 1), ('<groupId>commons-logging</groupId>', 1), ('<artifactId>jline</artifactId>', 1), ('<artifactId>avro</artifactId>', 1), ('<artifactId>libthrift</artifactId>', 1), ('<groupId>com.github.scopt</groupId>', 1), ('<artifactId>scopt_${scala.binary.version}</artifactId>', 1), ('<version>3.2.0</version>', 1), ('following', 1), ('are', 1), ('already', 1), ('Spark', 1), ('assembly,', 1), ('we', 1), ('them', 1), ('provided.', 1), ('</dependencies>', 2), ('<build>', 1), ('<outputDirectory>target/scala-${scala.binary.version}/classes</outputDirectory>', 1), ('<plugin>', 3), ('<groupId>org.apache.maven.plugins</groupId>', 3), ('<configuration>', 3), ('</plugin>', 3), ('<artifactId>maven-install-plugin</artifactId>', 1), ('<includes>', 1), ('</artifactSet>', 1), ('<filter>', 1), ('<exclude>META-INF/*.SF</exclude>', 1), ('<exclude>META-INF/*.RSA</exclude>', 1), ('</filter>', 1), ('<transformers>', 1), ('/>', 1), ('</transformer>', 2), ('implementation=\"org.apache.maven.plugins.shade.resource.DontIncludeResourceTransformer\">', 1), ('<resource>log4j.properties</resource>', 1), ('</transformers>', 1), ('<artifactId>spark-streaming-kinesis-asl_${scala.binary.version}</artifactId>', 1), ('</profile>', 6), ('Profiles', 1), ('disable', 1), ('inclusion', 1), ('certain', 1), ('dependencies.', 1), ('<flume.deps.scope>provided</flume.deps.scope>', 1), ('<id>hbase-provided</id>', 1), ('<hbase.deps.scope>provided</hbase.deps.scope>', 1), ('<id>parquet-provided</id>', 1), ('<parquet.deps.scope>provided</parquet.deps.scope>', 1), ('encoding=\"UTF-8\"?>', 1), ('<!--', 8), ('~', 14), ('Licensed', 1), ('to', 5), ('the', 10), ('Software', 1), ('Foundation', 1), ('(ASF)', 1), ('under', 4), ('one', 1), ('or', 3), ('contributor', 1), ('license', 1), ('agreements.', 1), ('See', 2), ('file', 3), ('distributed', 3), ('with', 2), ('for', 2), ('information', 1), ('ownership.', 1), ('ASF', 1), ('licenses', 1), ('You', 2), ('License,', 1), ('Version', 1), ('you', 1), ('not', 1), ('except', 1), ('a', 1), ('copy', 1), ('License', 3), ('http://www.apache.org/licenses/LICENSE-2.0', 1), ('Unless', 1), ('required', 1), ('by', 1), ('applicable', 1), ('agreed', 1), ('writing,', 1), ('software', 1), ('on', 1), ('WITHOUT', 1), ('WARRANTIES', 1), ('OR', 1), ('ANY', 1), ('either', 1), ('express', 1), ('implied.', 1), ('governing', 1), ('permissions', 1), ('and', 1), ('<project', 1), ('xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"', 1), ('xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0', 1), ('<parent>', 1), ('<groupId>org.apache.spark</groupId>', 14), ('<version>1.6.0-SNAPSHOT</version>', 1), ('<artifactId>spark-examples_2.10</artifactId>', 1), ('<properties>', 6), ('</properties>', 6), ('<packaging>jar</packaging>', 1), ('<name>Spark', 1), ('<url>http://spark.apache.org/</url>', 1), ('<dependency>', 25), ('<artifactId>spark-core_${scala.binary.version}</artifactId>', 1), ('<scope>provided</scope>', 8), ('</dependency>', 25), ('<artifactId>spark-mllib_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-twitter_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-mqtt_${scala.binary.version}</artifactId>', 1), ('<artifactId>spark-streaming-zeromq_${scala.binary.version}</artifactId>', 1), ('<artifactId>protobuf-java</artifactId>', 1), ('</exclusions>', 6), ('<artifactId>spark-streaming-kafka_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.hbase</groupId>', 12), ('<artifactId>hbase-testing-util</artifactId>', 1), ('<version>${hbase.version}</version>', 7), ('<scope>${hbase.deps.scope}</scope>', 6), ('<artifactId>hbase-annotations</artifactId>', 4), ('<groupId>org.jruby</groupId>', 1), ('<artifactId>jruby-complete</artifactId>', 1), ('<artifactId>hbase-client</artifactId>', 1), ('<artifactId>hbase-server</artifactId>', 1), ('<artifactId>hadoop-mapreduce-client-core</artifactId>', 1), ('<artifactId>hadoop-annotations</artifactId>', 1), ('<artifactId>hadoop-hdfs</artifactId>', 1), ('<groupId>org.apache.commons</groupId>', 3), ('<groupId>com.sun.jersey</groupId>', 4), ('<artifactId>slf4j-api</artifactId>', 1), ('hbase', 1), ('v2.4,', 1), ('which', 1), ('...-->', 1), ('<artifactId>hbase-hadoop-compat</artifactId>', 2), ('<type>test-jar</type>', 1), ('<artifactId>algebird-core_${scala.binary.version}</artifactId>', 1), ('<version>0.9.0</version>', 1), ('<artifactId>scalacheck_${scala.binary.version}</artifactId>', 1), ('<groupId>org.apache.cassandra</groupId>', 1), ('<groupId>com.google.guava</groupId>', 1), ('<groupId>com.ning</groupId>', 1), ('<artifactId>compress-lzf</artifactId>', 1), ('<groupId>commons-cli</groupId>', 1), ('<groupId>commons-codec</groupId>', 1), ('<artifactId>commons-codec</artifactId>', 1), ('<groupId>commons-lang</groupId>', 1), ('<artifactId>commons-lang</artifactId>', 1), ('<artifactId>commons-logging</artifactId>', 1), ('<groupId>jline</groupId>', 1), ('<groupId>net.jpountz.lz4</groupId>', 1), ('<artifactId>lz4</artifactId>', 1), ('<groupId>org.apache.cassandra.deps</groupId>', 1), ('<groupId>org.apache.thrift</groupId>', 1), ('dependencies', 1), ('present', 1), ('so', 1), ('want', 1), ('force', 1), ('be', 1), ('<groupId>org.scala-lang</groupId>', 1), ('<artifactId>scala-library</artifactId>', 1), ('<testOutputDirectory>target/scala-${scala.binary.version}/test-classes</testOutputDirectory>', 1), ('<plugins>', 1), ('<artifactId>maven-deploy-plugin</artifactId>', 1), ('<skip>true</skip>', 2), ('</configuration>', 3), ('<artifactId>maven-shade-plugin</artifactId>', 1), ('<shadedArtifactAttached>false</shadedArtifactAttached>', 1), ('<outputFile>${project.build.directory}/scala-${scala.binary.version}/spark-examples-${project.version}-hadoop${hadoop.version}.jar</outputFile>', 1), ('<artifactSet>', 1), ('<include>*:*</include>', 1), ('</includes>', 1), ('<filters>', 1), ('<artifact>*:*</artifact>', 1), ('<excludes>', 1), ('<exclude>META-INF/*.DSA</exclude>', 1), ('</excludes>', 1), ('</filters>', 1), ('<transformer', 3), ('implementation=\"org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\"', 1), ('implementation=\"org.apache.maven.plugins.shade.resource.AppendingTransformer\">', 1), ('<resource>reference.conf</resource>', 1), ('</plugins>', 1), ('</build>', 1), ('<profiles>', 1), ('<profile>', 6), ('<id>kinesis-asl</id>', 1), ('that', 1), ('<id>flume-provided</id>', 1), ('<id>hadoop-provided</id>', 1), ('<hadoop.deps.scope>provided</hadoop.deps.scope>', 1), ('<id>hive-provided</id>', 1), ('<hive.deps.scope>provided</hive.deps.scope>', 1), ('</profiles>', 1), ('</project>', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(pomCount.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método join combina dos RDDs/Datasets (K,V) y (K,W) juntos, obteniendose un RDD/Dataset de la forma (K, (V,W)).\n",
    "Usa el método con los RDDs anteriores (Readme y Pom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_rdd = readmeCount.join(pomCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra el valor del RDD resultante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', (1, 2)),\n",
       " ('Spark', (14, 1)),\n",
       " ('', (67, 2931)),\n",
       " ('is', (6, 2)),\n",
       " ('in', (5, 3)),\n",
       " ('an', (3, 1)),\n",
       " ('of', (5, 2)),\n",
       " ('this', (1, 3)),\n",
       " ('at', (2, 1)),\n",
       " ('The', (1, 2)),\n",
       " ('following', (2, 1)),\n",
       " ('use', (3, 1)),\n",
       " ('are', (1, 1)),\n",
       " ('uses', (1, 1)),\n",
       " ('a', (10, 1)),\n",
       " ('and', (10, 1)),\n",
       " ('for', (12, 2)),\n",
       " ('that', (3, 1)),\n",
       " ('You', (3, 2)),\n",
       " ('the', (21, 10)),\n",
       " ('on', (6, 1)),\n",
       " ('file', (1, 3)),\n",
       " ('not', (1, 1)),\n",
       " ('to', (14, 5)),\n",
       " ('you', (4, 1)),\n",
       " ('which', (2, 1)),\n",
       " ('with', (4, 2)),\n",
       " ('one', (2, 1)),\n",
       " ('be', (2, 1)),\n",
       " ('or', (3, 3)),\n",
       " ('See', (1, 2))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combina los valores de las cuentas de palabras (V,W) para obtener el valor total de las veces que aparece una palabra en los dos ficheros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joinedSum = joined_rdd.map(lambda k: (k[0], (k[1][0]+k[1][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobar si es correcto, imprime los primeros cinco elementos del RDD resultante de la unión de los dos originales y los cinco primeros elementos del último RDD calculado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', (1, 2)),\n",
       " ('Spark', (14, 1)),\n",
       " ('', (67, 2931)),\n",
       " ('is', (6, 2)),\n",
       " ('in', (5, 3))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_joined = joined_rdd.take(5)\n",
    "take_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apache', 3), ('Spark', 15), ('', 2998), ('is', 8), ('in', 8)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_sum = joinedSum.take(5)\n",
    "take_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast variables y acumuladores\n",
    "\n",
    "\n",
    "Más información: [http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables](http://spark.apache.org/docs/latest/programming-guide.html#broadcast-variables)\n",
    "\n",
    "Crear una variable de tipo broadcast con el array [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "broadcast_array = sc.broadcast([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra/obtén el valor de esa variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_array.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "\n",
    "Crea una variable de tipo acumulador con un valor inicial 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paraleliza un array de cuatro enteros y define una función que añada un valor entero a la variable de tipo acumulador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1,2,3,4]).foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muestra el valor de la variable de tipo acumulador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
